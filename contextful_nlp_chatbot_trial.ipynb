{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21Akame03/nlp_framework/blob/main/contextful_nlp_chatbot_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39PVpupK7mXm"
      },
      "source": [
        "## Json file for testing (just copy everything into a file named intents.json) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bE35Rsn7eNb"
      },
      "source": [
        "{\"intents\": [\n",
        "        {\"tag\": \"greeting\",\n",
        "         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\"],\n",
        "         \"responses\": [\"Hello, thanks for visiting\", \"Good to see you again\", \"Hi there, how can I help?\"],\n",
        "         \"context_set\": \"\"\n",
        "        },\n",
        "        {\"tag\": \"goodbye\",\n",
        "         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
        "         \"responses\": [\"See you later, thanks for visiting\", \"Have a nice day\", \"Bye! Come back again soon.\"]\n",
        "        },\n",
        "        {\"tag\": \"thanks\",\n",
        "         \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
        "         \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n",
        "        },\n",
        "        {\"tag\": \"hours\",\n",
        "         \"patterns\": [\"What hours are you open?\", \"What are your hours?\", \"When are you open?\" ],\n",
        "         \"responses\": [\"We're open every day 9am-9pm\", \"Our hours are 9am-9pm every day\"]\n",
        "        },\n",
        "        {\"tag\": \"mopeds\",\n",
        "         \"patterns\": [\"Which mopeds do you have?\", \"What kinds of mopeds are there?\", \"What do you rent?\" ],\n",
        "         \"responses\": [\"We rent Yamaha, Piaggio and Vespa mopeds\", \"We have Piaggio, Vespa and Yamaha mopeds\"]\n",
        "        },\n",
        "        {\"tag\": \"payments\",\n",
        "         \"patterns\": [\"Do you take credit cards?\", \"Do you accept Mastercard?\", \"Are you cash only?\" ],\n",
        "         \"responses\": [\"We accept VISA, Mastercard and AMEX\", \"We accept most major credit cards\"]\n",
        "        },\n",
        "        {\"tag\": \"opentoday\",\n",
        "         \"patterns\": [\"Are you open today?\", \"When do you open today?\", \"What are your hours today?\"],\n",
        "         \"responses\": [\"We're open every day from 9am-9pm\", \"Our hours are 9am-9pm every day\"]\n",
        "        },\n",
        "        {\"tag\": \"rental\",\n",
        "         \"patterns\": [\"Can we rent a moped?\", \"I'd like to rent a moped\", \"How does this work?\" ],\n",
        "         \"responses\": [\"Are you looking to rent today or later this week?\"],\n",
        "         \"context_set\": \"rentalday\"\n",
        "        },\n",
        "        {\"tag\": \"today\",\n",
        "         \"patterns\": [\"today\"],\n",
        "         \"responses\": [\"For rentals today please call 1-800-MYMOPED\", \"Same-day rentals please call 1-800-MYMOPED\"],\n",
        "         \"context_filter\": \"rentalday\"\n",
        "        }\n",
        "   ]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnvIBNYfysXa"
      },
      "source": [
        "1. a tag (a unique name)\n",
        "2. patterns (sentence patterns for our neural network text classifier)\n",
        "3. responses (one will be used as a response)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LCyffu-yY4n",
        "outputId": "5b083e56-5464-4c4b-a7cd-333e1871dc85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 107 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=043a61d7d528fe0f90555ad2a6b3434f82fd2f0192fdcf0eae8b640763123183\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU16A-2uysKd",
        "outputId": "692962e3-3028-4e63-ea04-ee81a9e1bb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Hello', 'Good morning'], 'responses': ['Hello sir, good to see you again', 'Good to see you again, sir', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later, sir', 'Have a nice day ahead, sir ', 'Bye!.']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\"], 'responses': ['Happy to help!', 'Any time!', 'It was my pleasure']}]}\n"
          ]
        }
      ],
      "source": [
        "# import json file for training\n",
        "import json\n",
        "with open('/content/intents.json') as json_data:\n",
        "  intents = json.loads(json_data.read())\n",
        "  print(intents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bbx43lU47R4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzZUKPuc1Q-v"
      },
      "source": [
        "## Organise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07CKn3WL0uQ1",
        "outputId": "9218e00c-ec95-4220-f4be-9aa9c6787c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 documents\n",
            "3 classes ['goodbye', 'greeting', 'thanks']\n",
            "15 unique stemmed words [\"'s\", 'ar', 'bye', 'good', 'goodby', 'hello', 'help', 'hi', 'how', 'lat', 'morn', 'see', 'thank', 'that', 'you']\n"
          ]
        }
      ],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    # tokenise each word in the sentence\n",
        "    token_words = nltk.word_tokenize(pattern)\n",
        "    # add to our word list\n",
        "    words.extend(token_words)\n",
        "    # add to documents in our corpus\n",
        "    documents.append((token_words, intent['tag']))\n",
        "    # add to classes list\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(words.lower()) for words in words if words not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4C6rLzB5bWD",
        "outputId": "d30bf67e-703c-460b-a047-00bc751766c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ],
      "source": [
        "# create training data\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "# empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# traininig set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "  # initialise bag of words\n",
        "  bag = []\n",
        "  # list of tokenised words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  # stem each word\n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  # create bag of words\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "  \n",
        "  # output is '0' for each tag and '1' for current tag\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "  training.append([bag, output_row])\n",
        "\n",
        "\n",
        "# shuffle features and turn it into np array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# training and test_list\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXB4CfBt9i4K",
        "outputId": "d0770706-1a9a-4330-aacb-8d100105ff6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.22623\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 1000 | loss: 0.22623 - acc: 0.9535 -- iter: 08/10\n",
            "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.20561\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 1000 | loss: 0.20561 - acc: 0.9582 -- iter: 10/10\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# define neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "# define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "\n",
        "# start training (gradient descent algo)\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4mUZZWlCXv-1"
      },
      "outputs": [],
      "source": [
        "# save all of our data structures\n",
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNMExJYs_e8e"
      },
      "source": [
        "## INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_XFGcIKSX1bN"
      },
      "outputs": [],
      "source": [
        "# restore all of our data structures\n",
        "import pickle\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "# import our chat-bot intents file\n",
        "import json\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNQv3LkGYIXD",
        "outputId": "2a1cd6cf-8fde-49be-9aad-161b642eea8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load our saved model\n",
        "model.load('./model.tflearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-b8eERu9d7hl"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  # tokenise input \n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  # stem each word\n",
        "  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exist in sentence (boolean like)\n",
        "def bow(sentence, words, show_details=True):\n",
        "  # tokenise the pattern\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "\n",
        "  # bag of words\n",
        "  bag = [0] * len(words)\n",
        "  for s in sentence_words:\n",
        "    for i, w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(f\"Found in bag: {w}\")\n",
        "  \n",
        "  return (np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gW2nM4yF2I0p"
      },
      "outputs": [],
      "source": [
        "ERROR_threshold = 0.25\n",
        "def classify(sentence):\n",
        "  # generate prob from model\n",
        "  results = model.predict([bow(sentence, words)])[0]\n",
        "\n",
        "  # filter out preds below threshold\n",
        "  results = [[i, r] for i, r in enumerate(results) if r > ERROR_threshold]\n",
        "\n",
        "  # sort by probability\n",
        "  results.sort(key = lambda x : x[1], reverse=True)\n",
        "  return_list = []\n",
        "\n",
        "  for r in results:\n",
        "    return_list.append((classes[r[0]], r[1]))\n",
        "  \n",
        "  # return tuple of intent and prob\n",
        "  return return_list\n",
        "\n",
        "# find a response \n",
        "def response(sentence, user=\"123\", show_details=True):\n",
        "  results = classify(sentence)\n",
        "  # if we have a classification then find the matching intent flag\n",
        "  if results:\n",
        "    # loop as long as there are matches to process\n",
        "    while results:\n",
        "      for i in intents['intents']:\n",
        "        # find tag matching the first result\n",
        "        if i['tag'] == results[0][0]:\n",
        "          # a random response from intent\n",
        "          return print(random.choice(i['responses']))\n",
        "      \n",
        "      results.pop(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IgS9b5j8pUS"
      },
      "source": [
        "## Contextualised version of response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "B5yoUEEy8o0T"
      },
      "outputs": [],
      "source": [
        "# # dict data structure\n",
        "# context = {}\n",
        "\n",
        "# def response(sentence, user=\"123\", show_details=True):\n",
        "#   results = classify(sentence)\n",
        "#   # if we have a classification then find the matching intent flag\n",
        "#   if results:\n",
        "#     # loop as long as there are matches to process\n",
        "#     while results:\n",
        "#       for i in intents['intents']:\n",
        "#         # find tag matching the first result\n",
        "#         if i['tag'] == results[0][0]:\n",
        "#           # set context for this intent if neccesary\n",
        "#           if 'context_set' in i:\n",
        "#             if show_details: print('context: ', i['context_set'])\n",
        "#             context[user] = i['context_set']\n",
        "\n",
        "#             # check if this intent is contextual and applies to this user's conversational\n",
        "#             if not 'context_filter' in i or \\\n",
        "#               (user in context and 'context_filter' in i and i['context_filter'] == context[user]):\n",
        "#               if show_details: print('tag: ', i['tag'])\n",
        "#               # a random response \n",
        "#               return print(random.choice(i['responses']))\n",
        "\n",
        "\n",
        "#       results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsPM1yvo-3-7",
        "outputId": "ed34afe4-8ae8-4e05-d4df-ed30f2f58bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found in bag: thank\n",
            "It was my pleasure\n"
          ]
        }
      ],
      "source": [
        "text = \"thanks\"\n",
        "# classify(text)\n",
        "response(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "contextful nlp chatbot trial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNEgSsLnt/Llqs2Q/J1ybm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}